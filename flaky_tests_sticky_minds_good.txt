-- What Flaky Tests Can Tell You -- 

Imagine this. You're a test developer who writes end-to-end automated tests for a web application. You use good technology in the Selenium WebDriver. You follow good practices like using page objects to separate the tests from the application logic. You make use of virtualization and use adequately powered machine hardware.  You also understand how end-to-end automation is a team effort and so you work with application developers, testers and managers in concert to get the most value out of your efforts. And yet for all your good work you still have a recurring problem: flaky tests. These are tests that pass or fail unexpectedly for reasons that appear random. Flaky tests become even worse as test suites grow as more areas of an app are covered and tested in detail. Eventually there's a temptation to throw up your hands and say "This is stupid!" and throw the whole test suite into the trash bin. 

But are flaky tests actually a problem?

It's easy to shrug off flaky tests and use them to discredit automated end-to-end testing. But flaky tests can be useful for learning more about an application under test and about how your team works. I'll provide some technical and human examples of where flaky tests can be helpful in software testing efforts. 

The Technical Side
From a technical standpoint, there's a few sources of flakiness in WebDriver-based tests. One of the main areas is synchronization. Web applications have many layers that affect performance, including network connection speed, browser HTTP handling and rendering and computer processing resources. As a result, some operations may take slightly longer (or shorter) than others to complete in an end-to-end scenario. A button may not appear in time, or a dialog box may not disappear fast enough for an automated script to complete without unexpected failures. The solution is to put wait statements to synchronize script steps with the application under test. This might seem like a hack to avoid flakiness, but it also may be an oracle of performance issues in your application. If some areas consistently need more waits or longer waits it could be an indication of poor performance, particularly client-side performance, in those areas. At one point on one team I worked with, there was one set of automated end-to-end tests that seemed to fail inconsistently all the time but related to the same feature. When I talked to developers, it turned out that area had some front-end problems due to some bad coding practices. Flaky tests sort of picked up on this problem if indirectly. 

Another problem I've seen producing flaky tests is from accidental load testing. As test suites grow, there isn't just more lines of test code there's also more tests being executed against the application under test. In turn, this usually means that test suites are re-organized to run in parallel or at least concurrently to help cut down on test runtime. While this is helpful for testers and developers, this can also have a side-effect of putting large loads on your application, creating an unintended load test. Test code that runs perfectly fine in series might have flakiness when run concurrently. In one case I saw on a project I was working on, some tests were run initially individually were working just fine but had some problems when we first tried running them in parallel, with a few (seemingly random) failures. After some debugging, one of my teammates found that when run in parallel, our tests would try all login with the same admin user instantly when tests started, resulting in around eight simultaneous logins by the same user. The application wasn't really prepared for this and we found out the hard way. It was helpful in the long run and helped us design better tests.

The Human Side
Writing automated end-to-end tests is a testing activity, it's important to think about how to use them from a testing perspective. One great application of flaky tests is use them as a barometer of teamwork and team communication. One challenge I've encountered several times is getting team members to take interest in end-to-end test results. Since flaky tests will sometimes appear to fail and other times appear to pass, interested team members - that is, people who are actually looking at test results - will ask about them. Even if your answer is "They're flaky", this is often a good place to start conversations about testing, quality and automation approaches. In my experience, if there's lots of flakiness and no one is asking about it, your team either isn't getting information about test results or isn't interested. Your team not getting information properly is a completely solvable problem, but one that's sometimes tricky to identify. It wasn't until I talked with one of the devs I've worked with on my current team that I found out application developers couldn't understand how to interpret our test output (via our CI server), and that issue partially arose because he was interested in why some of our tests were failing. If your team is completely indifferent to end-to-end automated test results completely, that's also solvable but requires a bit more creativity. 

Following along with gauging interest, flaky tests might also be able to tell you about "test results fatigue", a condition where teams are so inundated with test results that aren't totally reliable that they begin to ignore them. What starts off as a promising testing effort might eventually be ruined when team members ignore some flaky tests, then all tests related to the flaky tests, then effectively all tests. Test result fatigue is a scourge that can kill the benefit of automation, and a prime cause of test fatigue is flaky tests. Watching how your team reacts over time to flaky tests might give you insight to how much they're invested in automation over time, even past the honeymoon period of using any new tool. It might also give you some insight to how engaged overall your team is with a project at any given time.

Lastly, it's important to consider how automated end-to-end tests are being used in context. In a team or organization that practices continuous deployment, passing automated end-to-end tests might be a _requirement_ for product builds or releases. This means that flaky tests are a serious problem that need attention because they are halting releases. In other words, automated end-to-end tests are de facto acceptance tests (or if one prefers, rejection checks) and should be treated as such. Teams that use automated end-to-end tests as a regression testing approach - looking for known types of bugs before release - can approach flaky tests differently. Here tests can be interpreted by people and in turn, flaky tests can be interpreted accordingly. Both approaches are viable but it's important to understand _which approach_ your team is generally going for. 

Indeed, flaky tests can help answer the question of what a team is trying to accomplish overall with end-to-end automation and to further identify how to make their software better. 
