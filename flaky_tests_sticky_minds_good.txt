-- What Flaky Tests Can Tell You -- 

Imagine this. You're a test developer who writes end-to-end automated tests for a web application. You use good technology in the Selenium WebDriver. You follow good practices like using page objects to separate the tests from the application logic. You make use of virtualization and use adequately powered machine hardware.  You also understand how end-to-end automation is a team effort and so you work with application developers, testers and managers in concert to get the most value out of your efforts. And yet for all your good work you still have a recurring problem: flaky tests. These are tests that pass or fail unexpectedly for reasons that appear random. Flaky tests become even worse as test suites grow as more areas of an app are covered and tested in detail. Eventually there's a temptation to throw up your hands and say "This is stupid!" and throw the whole test suite into the trash bin. 

But are flaky tests actually a problem?

It's easy to shrug off flaky tests and use them to discredit automated end-to-end testing. But flaky tests can be useful for learning more about an application under test and about how your team works. I'll provide some technical and human examples of where flaky tests can be helpful in software testing efforts. 

The Technical Side
From a technical standpoint, there's a few sources of flakiness in WebDriver-based tests. One of the main areas is synchronization. Web applications have many layers that affect performance, including network connection speed, browser HTTP handling and rendering and computer processing resources. As a result, some operations may take slightly longer (or shorter) than others to complete in an end-to-end scenario. A button may not appear in time, or a dialog box may not disappear fast enough for an automated script to complete without unexpected failures. The solution is to put wait statements to synchronize script steps with the application under test. This might seem like a hack to avoid flakiness, but it also may be an oracle of performance issues in your application. If some areas consistently need more waits or longer waits it could be an indication of poor performance, particularly client-side performance, in those areas. At one point on one team I worked with, there was one set of automated end-to-end tests that seemed to fail inconsistently all the time but related to the same feature. When I talked to developers, it turned out that area had some front-end problems due to some bad coding practices. Flaky tests sort of picked up on this problem if indirectly. 

Another problem I've seen producing flaky tests is from accidental load testing. As test suites grow, there's more ...

The Human Side
Writing automated end-to-end tests is a testing activity, it's important to think about how to use them from a testing perspective. One great application of flaky tests is use them as a barometer of teamwork and team communication. One challenge I've encountered several times is getting team members to take interest in end-to-end test results. Since flaky tests will sometimes appear to fail and other times appear to pass, interested team members - that is, people who are actually looking at test results - will ask about them. Even if your answer is "They're flaky", this is often a good place to start conversations about testing, quality and automation approaches. In my experience, if there's lots of flakiness and no one is asking about it, your team either isn't getting information about test results or isn't interested. 

Following along with gauging interest, flaky tests might also be able to tell you about "test results fatigue", a condition where teams are so inundated with test results that aren't totally reliable that they begin to ignore them. Test result fatigue is a scourge that can kill the benefit of automation, and a prime cause of test fatigue is flaky tests. 

