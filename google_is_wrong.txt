I think Google is wrong. 

Specifically, I think Google is wrong about (automated end-to-end testing)[http://googletesting.blogspot.ca/2015/04/just-say-no-to-more-end-to-end-tests.html]. Of course, I'll add the disclaimer that Google is a large software company and so that post probably doesn't represent Google dev's opinion across the board. But Google's still wrong.

In that post, the author Mike Wacker discusses an idealized process for using end-to-end automated tests followed by how that process works in practice. The in-practice version discusses a situation where a goal of 90% pass rate is required for a project to be released. Due to a number of problems including hardware failures, slow build/test/refactor feedback cycles and flaky tests, the 90% pass rate is achieved late and only by fidgeting with tests. However even though the project is late the automated tests do find real bugs that are ultimately fixed before the product is released. 

The conclusion Mike comes to is that smaller, faster tests - unit tests - are preferable to end-to-end tests according to the well known testing pyramid. Essentially, the advice is have lots of unit tests and few (if any) end-to-end tests. Along the way, Mike slams end-to-end tests a bit pointed out many issues with them. But I think he's got things backwards. 

One thing I've learned about end-to-end automated tests is that their success or failure really (depend on the expectations people have around them)[http://simplythetest.tumblr.com/post/126611076050/a-few-observations]. How a team makes use of end-to-end tests is just as important as how well those tests are written. It's easy to write mediocre to poor end-to-end tests and difficult to write and maintain good ones. Good teams can still make use of the results from them in these cases.

Take Mike's hypothetical real-world scenario, where the 90% pass rate is a problem for the team because it's so difficult to reach it consistently. This was a team decision. If 90% was the goal, why not 85%? If the pass rate was changed, the project could've shipped a few days earlier. Maybe this would be preferable. 

As well, slow feedback loops were seen as a problem since tests only ran against nightly builds because they took so long. Is there a problem there? If tests could run against multiple daily builds perhaps the project could've been shipped sooner. Or test suites could've been run concurrently or on more hardware to reduce test run time. 

Another problem that's identified is a hardware failure that prevents a deployment from going forward and being tested. This could be a sign of much bigger issues like a poor deployment process or unstable builds. This could arguably be a significant problem, and it's a problem that end-to-end tests are surprisingly good at identifying. If all tests pass or aren't run, usually there's a significant failure in the app being built. Finding this could prevent downtime for a customer, a major blocker bug. Seeing all end-to-end tests suddenly fail or fail to even run is a really good oracle for general build/deployment problems. Typically developers and testers think of end-to-end tests being good at finding functional or user scenario bugs, but they can also be adept at finding bad builds. 

Lastly there's the usual complaint about flaky tests. I'm coming to the conclusion that tests that constantly fail _do_ fail for valid reasons. Constantly failed tests indicate something's going on with the app - minor or major - but it's usually highly context dependent. A human dev/tester needs to interpret these results to get good information out of failed test results. Flaky tests can also be indicators of subtle performance issues, but it takes some dedication to tease those issues out. I feel like in Mike's hypothetical example these end-to-end tests generate bug reports automatically which makes identifying issues in context more difficult. This also makes flaky tests even more difficult to work with.

I'm not saying that end-to-end testing is rainbows and unicorns. This post raises some legitimate problems with using them as part of a software project. But this post also alludes to how end-to-end tests can also be used as a tool to look for process and communication bugs in addition to end user bugs. If this kind of thinking is wrong, I don't want to be right.